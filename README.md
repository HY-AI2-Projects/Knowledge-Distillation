# Knowledge-Distillation
AI2 ê¸°ë§ í”„ë¡œì íŠ¸_ì •ë³´ìœµí•©ì „ê³µ 2020017210 ë°°ìœ ê²½

NIPS 2014 workshopì—ì„œ ë°œí‘œí•œ "Distilling the Knowledge in a Neural Network"ë…¼ë¬¸ê³¼ ë…¼ë¬¸êµ¬í˜„ ì½”ë“œì— ëŒ€í•´ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

## Knowledge Distillationì˜ ë“±ì¥ë°°ê²½
ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ëª¨ë¸ì€ ì ì  ë” ê¹Šì–´ì§€ê³  í¬ê¸°ê°€ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì—¬ëŸ¬ ë¬¸ì œì ì´ ë°œìƒí•©ë‹ˆë‹¤.

**1) ëª¨ë¸ë°°í¬ ë¬¸ì œ**

**2) ë¹„ìš©, ì¶”ë¡ ì‹œê°„ ì¦ê°€ ë¬¸ì œ**

**3) ì‹¤ìš©ì  ì¸¡ë©´ì—ì„œì˜ ë¬¸ì œ**

ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì—¬ëŸ¬ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ í•©ì³ì„œ í™œìš©í•˜ëŠ” ì•™ìƒë¸” ê¸°ë²•ì´ ëŒ€í‘œì ìœ¼ë¡œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì•„ë¬´ë¦¬ ë³µì¡í•œ ëª¨ë¸ì¼ì§€ë¼ë„ ìµœê³ ì˜ ì •í™•ë„ë¥¼ ë‚´ë„ë¡ ì„¤ê³„ë˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ëª¨ë¸ì€ ì‹¤ì œ ì‚¬ìš©ìì—ê²Œ ì í•©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë°”ì¼ì´ë‚˜ ìë™ì°¨ì™€ ê°™ì´ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” ì´ëŸ° ë³µì¡í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë˜ ì•™ìƒë¸” ê¸°ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ë†’ì•„ì§ˆ ìˆ˜ ìˆê² ì§€ë§Œ, ëŒ€ê·œëª¨ ëª¨ë¸ì€ ì—¬ëŸ¬ ëª¨ë¸ì´ í•©ì³ì§„ ë§Œí¼ ë¹„ìš©ì´ ê³¼ë„í•˜ê²Œ ë“¤ì–´ê°€ê³ , ê°™ì€ ìì›ì´ë¼ë©´ ì¶”ë¡ ì‹œê°„ì´ ì¦ê°€ í•  ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤. ê·¸ë ‡ê¸°ë•Œë¬¸ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼í•˜ëŠ” ì‘ì—…ì— ìˆì–´ì„œëŠ” ì ìš©ë˜ê¸°ê°€ í˜ë“­ë‹ˆë‹¤. ì´ì—ë”°ë¼ **ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ë” ë¹ ë¥´ê³  ê°€ë³ê²Œ í•  í•„ìš”ì„±ì„ ê°€ì§€ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.** ì´ë•Œ ë“±ì¥í•œ ê°œë…ì´ Knowledge Distillation ì…ë‹ˆë‹¤. 

## Knowledge Distillation ì´ë€?
í•œêµ­ì–´ë¡œ **'ì§€ì‹ ì¦ë¥˜'** ë¼ê³  í•˜ëŠ”ë°ìš”. ì¦ë¥˜ëŠ” ë¶ˆìˆœë¬¼ì´ ì„ì—¬ìˆëŠ” í˜¼í•©ë¬¼ì— ì˜¨ë„ë¥¼ ê°€í•˜ì—¬ ì›í•˜ëŠ” ì„±ë¶„ì„ ë¶„ë¦¬ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ê²ƒì„ ë”¥ëŸ¬ë‹ì— ì ìš©ì‹œí‚¤ë©´ **"ë¶ˆí•„ìš”í•˜ê²Œ ë§ì€ íŒŒë¼ë¯¸í„°ê°€ ì‚¬ìš©ë˜ëŠ” ê¸°ì¡´ì˜ ë³µì¡í•œ ëª¨ë¸ë“¤ë¡œë¶€í„°, ë³´ë‹¤ ë‹¨ìˆœí™”ëœ ëª¨ë¸ì— ì§€ì‹ì„ ì „ë‹¬í•´ì„œ í•µì‹¬ ë¶€ë¶„ì€ ì‚´ë¦¬ê³ , ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì€ ì œê±°í•˜ì—¬ ëª¨ë¸ì†ë„ë¥¼ ê°œì„ í•˜ëŠ” ê²ƒ"** ì…ë‹ˆë‹¤.

Knowledge Distillation ì—ëŠ” Teacher Modelê³¼ Student Modelì´ í•„ìš”í•©ë‹ˆë‹¤.
> Teacher Model (ì„ ìƒëª¨ë¸) : ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê°€ì§„ ë³µì¡í•œ ëª¨ë¸

 
> Student Model (í•™ìƒëª¨ë¸) : ì„ ìƒëª¨ë¸ì˜ ì§€ì‹ì„ ë°›ëŠ” ì–•ê³  ë‹¨ìˆœí•œ ëª¨ë¸

ì„ ìƒë‹˜ì´ í•™ìƒì—ê²Œ ê°€ë¥´ì¹¨ì„ í†µí•´ ì§€ì‹ì„ ì£¼ëŠ” ê²ƒ ì²˜ëŸ¼ **ì˜ í•™ìŠµëœ Teacher Modelì˜ ì§€ì‹ì„ Student Modelì—ê²Œ ì „ë‹¬í•˜ì—¬ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ê³ ì í•˜ëŠ” ê²ƒì´ Knowledge Distillaionì˜ ëª©ì ì…ë‹ˆë‹¤.**

![ìŠ¤í¬ë¦°ìƒ· 2023-12-13 001529](https://github.com/bae60/AI/assets/146174793/2d23749e-8045-4277-82b6-04f278b26baf) 
<p align="center">
[ê·¸ë¦¼1]
</p>


ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ì •í™•ë„ 95%, ì¶”ë¡ ì‹œê°„ 2ì‹œê°„ì¸ Teacher Modelì´ ìˆë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ ì˜ í•™ìŠµì‹œí‚¨ Teacher Modelì˜ ì§€ì‹ì„ ë‹¨ìˆœí•œ Student Modelì—ê²Œ ì „ë‹¬í•˜ì—¬ ì •í™•ë„ 90%, ì¶”ë¡ ì‹œê°„ 5ë¶„ì˜ ì„±ëŠ¥ì„ ë‚´ë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## Knowledge Distillaion ì ìš© ê³¼ì •
![image](https://github.com/bae60/AI/assets/146174793/f5d80690-9871-49c8-95f5-aa2707189f89)
<p align="center">
[ê·¸ë¦¼2]
</p>

**1) Soft Target**

![2ì œëª© ì—†ìŒ](https://github.com/bae60/AI/assets/146174793/4d47b99f-3873-4455-8fe9-a1c3ee8818f4)

ì¼ë°˜ì ì¸ ë¶„ë¥˜ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ê°’ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ë©´ ë§ˆì§€ë§‰ì— ë¡œì§“ê°’ì´ ì‚°ì¶œë˜ê²Œ ë©ë‹ˆë‹¤. ì—¬ê¸°ì— softmaxë¥¼ ì·¨í•˜ì—¬ í•©ì´ 1ì¸ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í˜•í•©ë‹ˆë‹¤. ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì„ ë³´ì´ëŠ” í´ë˜ìŠ¤ì—ëŠ” 1ì„ ë¶€ì—¬í•˜ê³  ë‚˜ë¨¸ì§€ì˜ í´ë˜ìŠ¤ì— ëŒ€í•´ì„œëŠ” 0ì„ ë¶€ì—¬í•˜ì—¬ ì•„ì›ƒí’‹ì„ ë„ì¶œí•˜ëŠ” One-hot ì¸ì½”ë”© ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **1ê³¼ 0ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì§„ ì˜ˆì¸¡ê°’ì„ Hard Target** ì´ë¼ê³  ì´ë¦„ ë¶™í˜”ìŠµë‹ˆë‹¤. Knowledge Distillaionì€ Hard Targetì´ ì•„ë‹Œ Soft Targetì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°ìš”.

![3ì œëª© ì—†ìŒ](https://github.com/bae60/AI/assets/146174793/170d806e-fdd1-4604-b87b-e6760e0ef40c)

**Soft Target ì€ One-hot ì¸ì½”ë”©ì„ ê±°ì¹˜ì§€ ì•Šì€ ì˜ˆì¸¡ê²°ê³¼ì˜ í™•ë¥ ë¶„í¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.** ìœ„ì˜ Soft Targetì„ ë³´ë©´ ì…ë ¥ì´ë¯¸ì§€ê°€ í† ë¼ë³´ë‹¤ëŠ” ê³ ì–‘ì´ì— ê°€ê¹ê³  ê³ ì–‘ì´ë³´ë‹¤ëŠ” ê°•ì•„ì§€ì— ë” ê°€ê¹ë‹¤ëŠ” ê²ƒ, ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ê°€ ì–´ëŠì •ë„ ë¹„ìŠ·í•œ íŠ¹ì§•ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ì ì—ì„œ ë´¤ì„ ë•Œ, ê¸°ì¡´ì˜ ì •ë‹µë°ì´í„°ì¸ Hard Targetë³´ë‹¤ Soft Targetì´ ê°–ëŠ” ì •ë³´ê°€ ë” í¬ë¯€ë¡œ ì´ ì§€ì‹ì„ Student Modelì—ê²Œ ì „ë‹¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ë ‡ê²Œ Soft Targetì„ í†µí•´ ì •ë³´ì†ì‹¤ì„ ìµœì†Œí™”í–ˆì§€ë§Œ ê°€ì¥ í° ë¡œì§“ê°’ì„ ê°–ëŠ” ë…¸ë“œì˜ ì¶œë ¥ê°’ì€ 1ê³¼ ë§¤ìš° ê°€ê¹Œìš´ ê°’ì„ ê°€ì§€ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ì— ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ ë§¤í•‘ë˜ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œì ì„ ê°œì„ í•˜ê¸° ìœ„í•´ **ğ‰(temperature)** ë¼ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ Softmax í•¨ìˆ˜ì— ì¶”ê°€í•˜ì—¬ ì•½ê°„ì˜ ë³€í˜•ì„ ì·¨í•´ì¤ë‹ˆë‹¤. ğ‰ëŠ” ì¼ì¢…ì˜ Scaling ì—­í• ì„ í•©ë‹ˆë‹¤. 
![4ì œëª© ì—†ìŒ](https://github.com/bae60/AI/assets/146174793/aa94c93f-155a-4155-81ac-e5d59b70c918)

**2) ì†ì‹¤í•¨ìˆ˜**

![image](https://github.com/bae60/AI/assets/146174793/d8f12b77-9752-48c1-aed7-25f68165dd10)
ğ‘† = Student model , ğ‘‡ = Teacher model 


ì•ì— ìˆ˜ì‹ì€ Distillaion lossë¥¼ êµ¬í•©ë‹ˆë‹¤. Sê°€ êµ¬í•œ soft prediction, Tê°€ êµ¬í•œ soft labelì˜ ì°¨ì´ë¡œ ì†ì‹¤ì„ êµ¬í•©ë‹ˆë‹¤. 
ë’¤ì— ìˆ˜ì‹ì€ ì¼ë°˜ì ì¸ ì‹ ê²½ë§ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ì†ì‹¤ì…ë‹ˆë‹¤. Sê°€ êµ¬í•œ soft predictionê³¼ ì›ë˜ ë°ì´í„°ê°€ ê°€ì§€ê³  ìˆëŠ” hard labelì˜ ì°¨ì´ë¡œ ì†ì‹¤ì„ êµ¬í•©ë‹ˆë‹¤. 
ì´ ë‘ê°€ì§€ ì†ì‹¤ì˜ í•©ì„ ìµœì¢… ì†ì‹¤ë¡œ ì‚¼ì•„ í•™ìŠµí•©ë‹ˆë‹¤.

## Knowledge Distillation ë…¼ë¬¸êµ¬í˜„
ë³¸ ì½”ë“œëŠ” https://github.com/Seonghoon-Yu/AI_Paper_Review/blob/master/Classification/Knowledge_distillation(2014).ipynb ì˜ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.

### ìš°ì„  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ import í•©ë‹ˆë‹¤.
```
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import torchvision.models as models
from torch.utils.data import DataLoader
import time
import os
import copy
from torchvision.transforms.functional import to_pil_image
import matplotlib.pyplot as plt
%matplotlib inline

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

### Loading MNIST dataset
MNINST datasetì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
```
# make directorch to save dataset
def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSerror:
        print('Error')
createFolder('./data')
```
```
# define transformation
ds_transform = transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,),(0.3081,))
])
```

datasetì„ ìƒì„±í•©ë‹ˆë‹¤.
```
# load MNIST dataset
train_ds = datasets.MNIST('/content/data',train=True, download=True, transform=ds_transform)
val_ds = datasets.MNIST('/content/data',train=False, download=True, transform=ds_transform)
```

ë°ì´ë” ë¡œë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
```
# define data loader
train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)
val_dl = DataLoader(val_ds, batch_size = 128, shuffle=True)
```

ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
```
# check sample image
for x, y in train_dl:
    print(x.shape, y.shape)
    break

num = 4
img = x[:num]

plt.figure(figsize=(15,15))
for i in range(num):
    plt.subplot(1,num+1,i+1)
    plt.imshow(to_pil_image(0.1307*img[i]+0.3081), cmap='gray')
```
torch.Size([64, 1, 28, 28]) torch.Size([64])
![download](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/27c5925f-2c79-4b0f-b72c-76ec61b121ae)

### Define Teacher model
Knowledge distillationì„ í•˜ê¸° ìœ„í•´ì„œ soft labelì„ ì–»ê¸° ìœ„í•œ teacher modelì„ ë¨¼ì € í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ teacher modelì„ ì •ì˜í•©ë‹ˆë‹¤.

```
class Teacher(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 1200)
        self.bn1 = nn.BatchNorm1d(1200)
        self.fc2 = nn.Linear(1200,1200)
        self.bn2 = nn.BatchNorm1d(1200)
        self.fc3 = nn.Linear(1200, 10)
    
    def forward(self,x):
        x = x.view(-1, 28*28)
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.dropout(x,p=0.8)
        x = F.relu(self.bn2(self.fc2(x)))
        x = F.dropout(x,p=0.8)
        x = self.fc3(x)
        return x
```

```
# check
x = torch.randn(16,1,28,28).to(device)
teacher = Teacher().to(device)
output = teacher(x)
print(output.shape)
```
<p align="center">
torch.Size([16, 10])
</p>



ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
```
# weight initialization
def initialize_weights(model):
    classname = model.__class__.__name__
    # fc layer
    if classname.find('Linear') != -1:
        nn.init.normal_(model.weight.data, 0.0, 0.02)
        nn.init.constant_(model.bias.data, 0)
    # batchnorm
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(model.weight.data, 1.0, 0.02)
        nn.init.constant_(model.bias.data, 0)

teacher.apply(initialize_weights);
```

### Train teacher model
teacher modelì„ í•™ìŠµí•©ë‹ˆë‹¤.
```
# loss function
loss_func = nn.CrossEntropyLoss()

# optimizer
opt = optim.Adam(teacher.parameters())

# lr scheduler
from torch.optim.lr_scheduler import ReduceLROnPlateau
lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)
```

```
# get current lr
def get_lr(opt):
    for param_group in opt.param_groups:
        return param_group['lr']


# calculate the metric per mini-batch
def metric_batch(output, target):
    pred = output.argmax(1, keepdim=True)
    corrects = pred.eq(target.view_as(pred)).sum().item()
    return corrects


# calculate the loss per mini-batch
def loss_batch(loss_func, output, target, opt=None):
    loss_b = loss_func(output, target)
    metric_b = metric_batch(output, target)

    if opt is not None:
        opt.zero_grad()
        loss_b.backward()
        opt.step()
    
    return loss_b.item(), metric_b


# calculate the loss per epochs
def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):
    running_loss = 0.0
    running_metric = 0.0
    len_data = len(dataset_dl.dataset)

    for xb, yb in dataset_dl:
        xb = xb.to(device)
        yb = yb.to(device)
        output = model(xb)

        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)

        running_loss += loss_b
        
        if metric_b is not None:
            running_metric += metric_b

        if sanity_check is True:
            break

    loss = running_loss / len_data
    metric = running_metric / len_data
    return loss, metric


# function to start training
def train_val(model, params):
    num_epochs=params['num_epochs']
    loss_func=params['loss_func']
    opt=params['optimizer']
    train_dl=params['train_dl']
    val_dl=params['val_dl']
    sanity_check=params['sanity_check']
    lr_scheduler=params['lr_scheduler']
    path2weights=params['path2weights']

    loss_history = {'train': [], 'val': []}
    metric_history = {'train': [], 'val': []}

    best_loss = float('inf')
    best_model_wts = copy.deepcopy(model.state_dict())
    start_time = time.time()

    for epoch in range(num_epochs):
        current_lr = get_lr(opt)
        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))

        model.train()
        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)
        loss_history['train'].append(train_loss)
        metric_history['train'].append(train_metric)

        model.eval()
        with torch.no_grad():
            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)
        loss_history['val'].append(val_loss)
        metric_history['val'].append(val_metric)

        if val_loss < best_loss:
            best_loss = val_loss
            best_model_wts = copy.deepcopy(model.state_dict())
            torch.save(model.state_dict(), path2weights)
            print('Copied best model weights!')

        lr_scheduler.step(val_loss)
        if current_lr != get_lr(opt):
            print('Loading best model weights!')
            model.load_state_dict(best_model_wts)

        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))
        print('-'*10)

    model.load_state_dict(best_model_wts)
    return model, loss_history, metric_history
```

```
# set hyper parameters
params_train = {
    'num_epochs':30,
    'optimizer':opt,
    'loss_func':loss_func,
    'train_dl':train_dl,
    'val_dl':val_dl,
    'sanity_check':False,
    'lr_scheduler':lr_scheduler,
    'path2weights':'./models/teacher_weights.pt',
}

createFolder('./models')
```

30 epoch í•™ìŠµí•©ë‹ˆë‹¤.
```
teacher, loss_hist, metric_hist = train_val(teacher, params_train)
```
![ìŠ¤í¬ë¦°ìƒ· 2023-12-16 193447](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/9e867c25-a9fb-4b55-aa44-a36899e815d8)
#### teacher modelì˜ accuracy 96.3 ë‚˜ì™”ìŠµë‹ˆë‹¤.


lossì™€ accuracyë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
```
num_epochs = params_train['num_epochs']

# Plot train-val loss
plt.title('Train-Val Loss')
plt.plot(range(1, num_epochs+1), loss_hist['train'], label='train')
plt.plot(range(1, num_epochs+1), loss_hist['val'], label='val')
plt.ylabel('Loss')
plt.xlabel('Training Epochs')
plt.legend()
plt.show()

# plot train-val accuracy
plt.title('Train-Val Accuracy')
plt.plot(range(1, num_epochs+1), metric_hist['train'], label='train')
plt.plot(range(1, num_epochs+1), metric_hist['val'], label='val')
plt.ylabel('Accuracy')
plt.xlabel('Training Epochs')
plt.legend()
plt.show()
```
![download](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/84c62a86-9826-422e-9fd8-b40bc2c65c5b)
![download](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/7d110750-68f2-44a3-b19f-13bd6ac7a52d)

### Define Student model
ì´ì œ teacherì˜ ì§€ì‹ì„ transferí•  student modelì„ ì •ì˜í•©ë‹ˆë‹¤.
```
class Student(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 800)
        self.bn1 = nn.BatchNorm1d(800)
        self.fc2 = nn.Linear(800,800)
        self.bn2 = nn.BatchNorm1d(800)
        self.fc3 = nn.Linear(800,10)

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x
```

```
# check
x = torch.randn(16,1,28,28).to(device)
student = Student().to(device)
output = student(x)
print(output.shape)
```
<p align="center">
torch.Size([16, 10])
</p>


ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
```
# weight initialization
def initialize_weights(model):
    classname = model.__class__.__name__
    # fc layer
    if classname.find('Linear') != -1:
        nn.init.normal_(model.weight.data, 0.0, 0.02)
        nn.init.constant_(model.bias.data, 0)
    # batchnorm
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(model.weight.data, 1.0, 0.02)
        nn.init.constant_(model.bias.data, 0)

student.apply(initialize_weights);
```

### Knowledge distillation
ì´ì œ ì•ì„œ ì„¤ëª…ë“œë¦° teacher modelì˜ soft labelì„ ì‚¬ìš©í•˜ì—¬ student modelì„ knowledge distillation lossë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
```
teacher = Teacher().to(device)
# load weight
teacher.load_state_dict(torch.load('/content/models/teacher_weights.pt'))

student = Student().to(device)

# optimizer
opt = optim.Adam(student.parameters())
```

```
# knowledge distillation loss
def distillation(y, labels, teacher_scores, T, alpha):
    # distillation loss + classification loss
    # y: student
    # labels: hard label
    # teacher_scores: soft label
    return nn.KLDivLoss()(F.log_softmax(y/T), F.softmax(teacher_scores/T)) * (T*T * 2.0 + alpha) + F.cross_entropy(y,labels) * (1.-alpha)

# val loss
loss_func = nn.CrossEntropyLoss()
```

```
def distill_loss_batch(output, target, teacher_output, loss_fn=distillation, opt=opt):
    loss_b = loss_fn(output, target, teacher_output, T=20.0, alpha=0.7)
    metric_b = metric_batch(output, target)

    if opt is not None:
        opt.zero_grad()
        loss_b.backward()
        opt.step()

    return loss_b.item(), metric_b
```

100epoch í•™ìŠµí•˜ê² ìŠµë‹ˆë‹¤.
```
num_epochs= 100

loss_history = {'train': [], 'val': []}
metric_history = {'train': [], 'val': []}

best_loss = float('inf')
start_time = time.time()

for epoch in range(num_epochs):
    current_lr = get_lr(opt)
    print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))

    # train
    student.train()

    running_loss = 0.0
    running_metric = 0.0
    len_data = len(train_dl.dataset)

    for xb, yb in train_dl:
        xb = xb.to(device)
        yb = yb.to(device)

        output = student(xb)
        teacher_output = teacher(xb).detach()
        loss_b, metric_b = distill_loss_batch(output, yb, teacher_output, loss_fn=distillation, opt=opt)
        running_loss += loss_b
        running_metric_b = metric_b
    train_loss = running_loss / len_data
    train_metric = running_metric / len_data

    loss_history['train'].append(train_loss)
    metric_history['train'].append(train_metric)

    # validation
    student.eval()
    with torch.no_grad():
        val_loss, val_metric = loss_epoch(student, loss_func, val_dl)
    loss_history['val'].append(val_loss)
    metric_history['val'].append(val_metric)


    lr_scheduler.step(val_loss)

    print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))
    print('-'*10)
```
![ìŠ¤í¬ë¦°ìƒ· 2023-12-16 200619](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/37505b5d-fd16-41d8-a490-928adc691186)

#### teacher modelë³´ë‹¤ accuracyê°€ 1.88% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤ !

lossì™€ accuracyë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
```
# Plot train-val loss
plt.title('Train-Val Loss')
plt.plot(range(1, num_epochs+1), loss_hist['train'], label='train')
plt.plot(range(1, num_epochs+1), loss_hist['val'], label='val')
plt.ylabel('Loss')
plt.xlabel('Training Epochs')
plt.legend()
plt.show()

# plot train-val accuracy
plt.title('Train-Val Accuracy')
plt.plot(range(1, num_epochs+1), metric_hist['train'], label='train')
plt.plot(range(1, num_epochs+1), metric_hist['val'], label='val')
plt.ylabel('Accuracy')
plt.xlabel('Training Epochs')
plt.legend()
plt.show()
```
![download](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/c12d6fe6-0d2d-4a77-848a-e871e456c1e4)
![download](https://github.com/HY-AI2-Projects/Knowledge-Distillation/assets/146174793/c5ea590a-bb97-4716-bb5b-fab9cd31b2b8)

## ê²°ë¡ 
* í° ëª¨ë¸ì˜ ì§€ì‹ì„ ì‘ì€ ë„¤íŠ¸ì›Œí¬ì— ì „ë‹¬ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ Knowledge distillationì…ë‹ˆë‹¤.
* Knowledge distillation êµ¬í˜„ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ë©° ì‹¤ì œ ë”¥ëŸ¬ë‹ ì„œë¹„ìŠ¤ì—ì„œ ìœ ìš©í•˜ê²Œ í™œìš©ë  ìˆ˜ ìˆìŒì„ ì•Œì•˜ìŠµë‹ˆë‹¤.

## ì°¸ê³ ìë£Œ
* [Knowledge distillation ë…¼ë¬¸](https://ffighting.net/deep-learning-paper-review/deep-learning-paper-guide/deep-learning-paper-guide/#Knowledge_Distillation)
* [ê·¸ë¦¼1](https://velog.io/@dldydldy75)
* [ê·¸ë¦¼2](https://intellabs.github.io/distiller/knowledge_distillation.html)
